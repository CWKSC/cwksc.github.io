---
title: Explain ZH
---

# 問題 3.8(d) 解釋

## 眾數 vs 均值

* **MAP** 估計後驗的 **眾數 (Mode)**。
* **貝葉斯預測 (來自 c 部分)** 使用後驗的 **均值 (Mean)**。

對於 Beta 分佈 $\text{Beta}(\alpha, \beta)$：
* 眾數 = $\frac{\alpha-1}{\alpha+\beta-2}$
* 均值 = $\frac{\alpha}{\alpha+\beta}$

使用均勻先驗 ($\alpha=1, \beta=1$) 和數據 ($s, n-s$)：
後驗為 $\text{Beta}(s+1, n-s+1)$。
* $\alpha_{post} = s+1$
* $\beta_{post} = n-s+1$

**MAP (眾數):**
$$
\frac{(s+1)-1}{(s+1)+(n-s+1)-2} = \frac{s}{n+2-2} = \frac{s}{n}
$$
(嚴格來說，這僅在計數 > 1 時有定義，但極限成立)。

**貝葉斯估計量 (均值):**
$$
\frac{s+1}{(s+1)+(n-s+1)} = \frac{s+1}{n+2}
$$

## 為什麼這裡 MAP 等於 ML？

MAP 是 ML 乘以先驗。如果先驗是平坦的（乘以 1），景觀中的“山丘”完全由似然函數定義。所以峰值（眾數）在同一地點。

## 實際意義

在機器學習中，我們通常更喜歡貝葉斯均值（或平滑估計），因為預測恰好為 0 或 1 是危險的。如果你估計某個事件的概率為 0，而它發生了，你的誤差（對數損失）是無限的。貝葉斯估計通過對不確定性進行積分，自然地防止了這種情況。
