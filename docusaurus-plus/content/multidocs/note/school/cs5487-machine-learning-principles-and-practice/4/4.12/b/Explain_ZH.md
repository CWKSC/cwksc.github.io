---
title: Explain ZH
---

## 解說

在 (a) 部分，目標函數 $\sum N_j \log \pi_j$ 導致的解中，$\pi_j$ 與 $N_j$ 成線性正比。這是多項分佈標準 MLE 的特徵。

在 (b) 部分，目標函數略有不同：$\sum \pi_j (N_j - \log \pi_j)$。這看起來像是一個熵 (entropy) 項 ($\pi \log \pi$) 結合一個線性項 ($\pi N$)。

當我們在總和約束 $\sum \pi_j = 1$ 下最大化這個函數時：

1. **似然 vs 熵**：$-\pi_j \log \pi_j$ 項是熵。最大化熵通常傾向於均勻分佈。$\pi_j N_j$ 項則根據 $N_j$ 對機率進行加權。
2. **指數關係**：
    * $\log x$ 的導數是 $1/x$。
    * $x \log x$ 的導數是 $1 + \log x$。
    * 因為目標函數具有 $\pi \log \pi$ 的形式，其導數包含 $\log \pi$ 項（沒有 (a) 部分中看到的 $1/\pi$ 縮放）。
    * 為了解決 $\log \pi = C$（其中 C 是源自其他項的常數），我們必須使用指數函數：$\pi = e^C$。
3. **Softmax 函數**：結果形式 $\pi_j = \frac{\exp(N_j)}{\sum \exp(N_k)}$ 即著名的 **Softmax 函數**。它將一個實數向量 $N$ 轉換為與輸入數值的指數成正比的機率分佈。這廣泛應用於神經網路和機器學習中，用於將 logit 轉換為機率。
